{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf1dc7",
   "metadata": {
    "id": "5faf1dc7"
   },
   "outputs": [],
   "source": [
    "! pip install IPython\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793331ac",
   "metadata": {
    "id": "793331ac"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bd9404",
   "metadata": {
    "id": "96bd9404"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039ac48b",
   "metadata": {
    "id": "039ac48b"
   },
   "outputs": [],
   "source": [
    "#import splitfolders\n",
    "\n",
    "# train, test split\n",
    "#splitfolders.ratio('brain_tumor_dataset', output=\"brain_tumor_dataset1\", ratio=(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4882d8a",
   "metadata": {
    "id": "f4882d8a",
    "outputId": "7339b20e-9080-4e6a-ab8b-f7eda4fd626f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7845 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('facial_exp/train',\n",
    "                                                 target_size = (256,256),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a998a05",
   "metadata": {
    "id": "9a998a05",
    "outputId": "f02e4fea-ca1f-45a4-a1c0-eda177f0dcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True)\n",
    "test_set = test_datagen.flow_from_directory('facial_exp/validation',\n",
    "                                            target_size = (256, 256),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b8be74",
   "metadata": {
    "id": "d3b8be74",
    "outputId": "541cca37-da9d-4128-8b41-1c0fe9ec685e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25defbc6ac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv.imread(cv.samples.findFile(\"facial_exp/train/angry/0.jpg\"))\n",
    "\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca809bda",
   "metadata": {
    "id": "ca809bda",
    "outputId": "98a4ec45-7d09-46eb-c448-f5d8f57ad3bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=os.listdir('facial_exp/train/angry')\n",
    "x2=os.listdir('facial_exp/train/disgust')\n",
    "x3=os.listdir('facial_exp/train/fear')\n",
    "x4=os.listdir('facial_exp/train/happy')\n",
    "x5=os.listdir('facial_exp/train/neutral')\n",
    "\"\"\"x6=os.listdir('facial_exp/train/sad')\n",
    "x7=os.listdir('facial_exp/train/surprise')\"\"\"\n",
    "train_x_data=np.concatenate([x1,x2,x3,x4,x5])\n",
    "len(train_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428962e1",
   "metadata": {
    "id": "428962e1",
    "outputId": "141fff4b-0776-4cf1-a23e-b8d9a64c3b22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_x1 = np.full(len(x1),0)\n",
    "target_x2 = np.full(len(x2),1)\n",
    "target_x3 = np.full(len(x3),2)\n",
    "target_x4 = np.full(len(x4),3)\n",
    "target_x5 = np.full(len(x5),4)\n",
    "\"\"\"target_x6 = np.full(len(x6),5)\n",
    "target_x7 = np.full(len(x7),6)\"\"\"\n",
    "train_y_data = np.concatenate([target_x1,target_x2,target_x3,target_x4,target_x5])\n",
    "len(train_y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a056345",
   "metadata": {
    "id": "0a056345",
    "outputId": "29aa1a9b-edd0-4efb-f524-a2ddb1850d9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y_data)==len(train_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdbe6805",
   "metadata": {
    "id": "bdbe6805",
    "outputId": "06114b92-3be3-4779-b833-79368099dc02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5130"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx1=os.listdir('facial_exp/validation/angry')\n",
    "tx2=os.listdir('facial_exp/validation/disgust')\n",
    "tx3=os.listdir('facial_exp/validation/fear')\n",
    "tx4=os.listdir('facial_exp/validation/happy')\n",
    "tx5=os.listdir('facial_exp/validation/neutral')\n",
    "\"\"\"tx6=os.listdir('facial_exp/validation/sad')\n",
    "tx7=os.listdir('facial_exp/validation/surprise')\"\"\"\n",
    "test_x_data=np.concatenate([tx1,tx2,tx3,tx4,tx5])\n",
    "len(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f2dae0",
   "metadata": {
    "id": "24f2dae0",
    "outputId": "0c58a3c4-63b4-4216-a1b1-8c73f89e18c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5130"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tx1 = np.full(len(tx1),0)\n",
    "target_tx2 = np.full(len(tx2),1)\n",
    "target_tx3 = np.full(len(tx3),2)\n",
    "target_tx4 = np.full(len(tx4),3)\n",
    "target_tx5 = np.full(len(tx5),4)\n",
    "\"\"\"target_tx6 = np.full(len(tx6),5)\n",
    "target_tx7 = np.full(len(tx7),6)\"\"\"\n",
    "test_y_data = np.concatenate([target_tx1,target_tx2,target_tx3,target_tx4,target_tx5])\n",
    "len(test_y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc1cd82",
   "metadata": {
    "id": "1dc1cd82",
    "outputId": "9cc34c01-c282-4b00-aae6-2ab99ec8dcdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y_data)==len(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2295564",
   "metadata": {
    "id": "c2295564",
    "outputId": "ca7d9d24-284e-419b-95cc-1db652b1b2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6103"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a5ee84",
   "metadata": {
    "id": "d1a5ee84",
    "outputId": "105ce3d8-b68e-4978-e192-0d9ed2e07297",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2b8d80c",
   "metadata": {
    "id": "d2b8d80c",
    "outputId": "e97c0e9b-fdfa-403b-c733-d4f40a573d8d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5130"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0086b85a",
   "metadata": {
    "id": "0086b85a",
    "outputId": "feecead9-7911-4be9-f305-74b3d705791f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ccb093e",
   "metadata": {
    "id": "6ccb093e"
   },
   "outputs": [],
   "source": [
    "x1_values=os.listdir('facial_exp/train/angry')\n",
    "x2_values=os.listdir('facial_exp/train/disgust')\n",
    "x3_values=os.listdir('facial_exp/train/fear')\n",
    "x4_values=os.listdir('facial_exp/train/happy')\n",
    "x5_values=os.listdir('facial_exp/train/neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8468292f",
   "metadata": {
    "id": "8468292f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for file in x6_values:\\n    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\\n    img = cv.imread('facial_exp/train/sad/'+file)\\n    face = cv.resize(img, (64, 64) )\\n    (b, g, r)=cv.split(face) \\n    img=cv.merge([r,g,b])\\n    X_data.append(img)\\nfor file1 in x7_values:\\n    img = cv.imread('facial_exp/train/surprise/'+file1)\\n    face = cv.resize(img, (64, 64) )\\n    (b, g, r)=cv.split(face) \\n    img=cv.merge([r,g,b])\\n    X_data.append(img)\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data =[]\n",
    "for file in x1_values:\n",
    "    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\n",
    "    img = cv.imread('facial_exp/train/angry/'+file)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\n",
    "for file1 in x2_values:\n",
    "    img = cv.imread('facial_exp/train/disgust/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\n",
    "for file in x3_values:\n",
    "    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\n",
    "    img = cv.imread('facial_exp/train/fear/'+file)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\n",
    "for file1 in x4_values:\n",
    "    img = cv.imread('facial_exp/train/happy/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\n",
    "for file1 in x5_values:\n",
    "    img = cv.imread('facial_exp/train/neutral/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\n",
    "\"\"\"for file in x6_values:\n",
    "    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\n",
    "    img = cv.imread('facial_exp/train/sad/'+file)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\n",
    "for file1 in x7_values:\n",
    "    img = cv.imread('facial_exp/train/surprise/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    X_data.append(img)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d642d374",
   "metadata": {
    "id": "d642d374",
    "outputId": "43304196-7efb-4b83-af66-641fb24dbfec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6103, 64, 64, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = np.squeeze(X_data)\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa3903ee",
   "metadata": {
    "id": "fa3903ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tx6_values=os.listdir('facial_exp/validation/sad')\\ntx7_values=os.listdir('facial_exp/validation/surprise')\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx1_values=os.listdir('facial_exp/validation/angry')\n",
    "tx2_values=os.listdir('facial_exp/validation/disgust')\n",
    "tx3_values=os.listdir('facial_exp/validation/fear')\n",
    "tx4_values=os.listdir('facial_exp/validation/happy')\n",
    "tx5_values=os.listdir('facial_exp/validation/neutral')\n",
    "\"\"\"tx6_values=os.listdir('facial_exp/validation/sad')\n",
    "tx7_values=os.listdir('facial_exp/validation/surprise')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e581fdb",
   "metadata": {
    "id": "5e581fdb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for file in tx6_values:\\n    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\\n    img = cv.imread('facial_exp/validation/sad/'+file)\\n    face = cv.resize(img, (64, 64) )\\n    (b, g, r)=cv.split(face) \\n    img=cv.merge([r,g,b])\\n    tX_data.append(img)\\nfor file1 in tx7_values:\\n    img = cv.imread('facial_exp/validation/surprise/'+file1)\\n    face = cv.resize(img, (64, 64) )\\n    (b, g, r)=cv.split(face) \\n    img=cv.merge([r,g,b])\\n    tX_data.append(img)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_data =[]\n",
    "for file in tx1_values:\n",
    "    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\n",
    "    img = cv.imread('facial_exp/validation/angry/'+file)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\n",
    "for file1 in tx2_values:\n",
    "    img = cv.imread('facial_exp/validation/disgust/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\n",
    "for file in tx3_values:\n",
    "    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\n",
    "    img = cv.imread('facial_exp/validation/fear/'+file)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\n",
    "for file1 in tx4_values:\n",
    "    img = cv.imread('facial_exp/validation/happy/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\n",
    "for file1 in tx5_values:\n",
    "    img = cv.imread('facial_exp/validation/neutral/'+file1)\n",
    "    face = cv.resize(img, (64,64))\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\n",
    "\"\"\"for file in tx6_values:\n",
    "    #face = misc.imread('../input/brain_tumor_dataset/yes/'+file)\n",
    "    img = cv.imread('facial_exp/validation/sad/'+file)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\n",
    "for file1 in tx7_values:\n",
    "    img = cv.imread('facial_exp/validation/surprise/'+file1)\n",
    "    face = cv.resize(img, (64, 64) )\n",
    "    (b, g, r)=cv.split(face) \n",
    "    img=cv.merge([r,g,b])\n",
    "    tX_data.append(img)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd5b68a4",
   "metadata": {
    "id": "fd5b68a4",
    "outputId": "067e23b8-349f-4ad1-c48b-2070b715682c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5130, 64, 64, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX1 = np.squeeze(tX_data)\n",
    "tX1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf039d26",
   "metadata": {
    "id": "cf039d26",
    "outputId": "8b8479d6-1c8a-44a6-938d-b5051bff8fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6103, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "X = np.squeeze(X1)\n",
    "print(X.shape)\n",
    "#testing\n",
    "#X1 = np.squeeze(X_test_data)\n",
    "#print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc540b14",
   "metadata": {
    "id": "cc540b14"
   },
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "X /= 255\n",
    "tX1 = tX1.astype('float32')\n",
    "tX1 /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27725f99",
   "metadata": {
    "id": "27725f99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5130"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = (X, train_y_data) , (tX1 , test_y_data )\n",
    "#(x_valid , y_valid) = (x_test[:63], y_test[:63])\n",
    "(x_valid , y_valid) = (x_test, y_test)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f67ea0",
   "metadata": {
    "id": "b6f67ea0"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "512e9573",
   "metadata": {
    "id": "512e9573"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=x_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46deabb",
   "metadata": {
    "id": "f46deabb"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67cc2e78",
   "metadata": {
    "id": "67cc2e78"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89c1f077",
   "metadata": {
    "id": "89c1f077"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b94bb9d",
   "metadata": {
    "id": "6b94bb9d"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0befbe4",
   "metadata": {
    "id": "d0befbe4",
    "outputId": "2fef46be-e5b1-42d4-f3e7-6e5bd63179ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 823,845\n",
      "Trainable params: 823,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=5, activation='sigmoid'))\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d96afe73",
   "metadata": {
    "id": "d96afe73"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17db0caf",
   "metadata": {
    "id": "17db0caf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rithv\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4929: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 42s 211ms/step - loss: 1.5215 - accuracy: 0.2984 - val_loss: 1.4907 - val_accuracy: 0.3273\n",
      "Epoch 2/25\n",
      "191/191 [==============================] - 39s 206ms/step - loss: 1.4165 - accuracy: 0.3965 - val_loss: 1.3878 - val_accuracy: 0.4526\n",
      "Epoch 3/25\n",
      "191/191 [==============================] - 39s 205ms/step - loss: 1.3363 - accuracy: 0.4454 - val_loss: 1.3797 - val_accuracy: 0.4193\n",
      "Epoch 4/25\n",
      "191/191 [==============================] - 40s 210ms/step - loss: 1.2657 - accuracy: 0.4773 - val_loss: 1.2609 - val_accuracy: 0.4774\n",
      "Epoch 5/25\n",
      "191/191 [==============================] - 40s 209ms/step - loss: 1.1801 - accuracy: 0.5161 - val_loss: 1.2214 - val_accuracy: 0.4990\n",
      "Epoch 6/25\n",
      "191/191 [==============================] - 40s 207ms/step - loss: 1.0866 - accuracy: 0.5620 - val_loss: 1.2973 - val_accuracy: 0.4671\n",
      "Epoch 7/25\n",
      "191/191 [==============================] - 39s 206ms/step - loss: 0.9751 - accuracy: 0.6135 - val_loss: 1.2822 - val_accuracy: 0.4959\n",
      "Epoch 8/25\n",
      "191/191 [==============================] - 40s 212ms/step - loss: 0.8513 - accuracy: 0.6708 - val_loss: 1.3064 - val_accuracy: 0.4914\n",
      "Epoch 9/25\n",
      "191/191 [==============================] - 40s 209ms/step - loss: 0.7038 - accuracy: 0.7342 - val_loss: 1.4493 - val_accuracy: 0.5062\n",
      "Epoch 10/25\n",
      "191/191 [==============================] - 39s 207ms/step - loss: 0.5439 - accuracy: 0.8001 - val_loss: 1.7238 - val_accuracy: 0.4846\n",
      "Epoch 11/25\n",
      "191/191 [==============================] - 39s 204ms/step - loss: 0.4178 - accuracy: 0.8553 - val_loss: 1.8850 - val_accuracy: 0.4864\n",
      "Epoch 12/25\n",
      "191/191 [==============================] - 38s 201ms/step - loss: 0.2996 - accuracy: 0.8987 - val_loss: 2.2020 - val_accuracy: 0.4805\n",
      "Epoch 13/25\n",
      "191/191 [==============================] - 40s 209ms/step - loss: 0.2004 - accuracy: 0.9369 - val_loss: 2.4239 - val_accuracy: 0.4686\n",
      "Epoch 14/25\n",
      "191/191 [==============================] - 40s 207ms/step - loss: 0.1345 - accuracy: 0.9626 - val_loss: 2.9601 - val_accuracy: 0.4661\n",
      "Epoch 15/25\n",
      "191/191 [==============================] - 40s 209ms/step - loss: 0.0975 - accuracy: 0.9739 - val_loss: 3.0929 - val_accuracy: 0.4696\n",
      "Epoch 16/25\n",
      "191/191 [==============================] - 40s 208ms/step - loss: 0.0693 - accuracy: 0.9831 - val_loss: 3.4527 - val_accuracy: 0.4667\n",
      "Epoch 17/25\n",
      "191/191 [==============================] - 40s 209ms/step - loss: 0.0515 - accuracy: 0.9897 - val_loss: 3.5940 - val_accuracy: 0.4982\n",
      "Epoch 18/25\n",
      "191/191 [==============================] - 40s 208ms/step - loss: 0.0426 - accuracy: 0.9907 - val_loss: 3.6301 - val_accuracy: 0.4936\n",
      "Epoch 19/25\n",
      "191/191 [==============================] - 40s 211ms/step - loss: 0.0553 - accuracy: 0.9862 - val_loss: 3.7843 - val_accuracy: 0.4871\n",
      "Epoch 20/25\n",
      "191/191 [==============================] - 40s 209ms/step - loss: 0.1013 - accuracy: 0.9702 - val_loss: 3.6834 - val_accuracy: 0.4550\n",
      "Epoch 21/25\n",
      "191/191 [==============================] - 40s 207ms/step - loss: 0.0727 - accuracy: 0.9780 - val_loss: 3.8014 - val_accuracy: 0.4766\n",
      "Epoch 22/25\n",
      "191/191 [==============================] - 39s 204ms/step - loss: 0.0347 - accuracy: 0.9926 - val_loss: 4.1587 - val_accuracy: 0.4823\n",
      "Epoch 23/25\n",
      "191/191 [==============================] - 40s 207ms/step - loss: 0.0234 - accuracy: 0.9956 - val_loss: 4.2644 - val_accuracy: 0.4848\n",
      "Epoch 24/25\n",
      "191/191 [==============================] - 39s 204ms/step - loss: 0.0119 - accuracy: 0.9979 - val_loss: 4.3888 - val_accuracy: 0.4959\n",
      "Epoch 25/25\n",
      "191/191 [==============================] - 39s 205ms/step - loss: 0.0269 - accuracy: 0.9930 - val_loss: 4.5904 - val_accuracy: 0.4772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25e0e7d1d60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train, y_train,validation_data=(x_test,y_test), epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa6b27",
   "metadata": {
    "id": "18aa6b27"
   },
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "class_names = [c.strip() for c in open(\"Moods.txt\").readlines()]\n",
    "count =0\n",
    "while(True):\n",
    "    ret, image = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    img = cv.resize(image, (32, 32))\n",
    "    (b1, g1, r1) = cv.split(image)\n",
    "    image=cv.merge([r1,g1,b1])\n",
    "    (b, g, r)=cv.split(img) \n",
    "    img=cv.merge([r,g,b])\n",
    "    a = np.squeeze(img)\n",
    "    a = a[np.newaxis, :, :]\n",
    "    pred_y = cnn.predict(a)\n",
    "    font = cv.FONT_HERSHEY_SIMPLEX\n",
    "    if(pred_y[0][0]>pred_y[0][1] and pred_y[0][0]>pred_y[0][2] and pred_y[0][0]>pred_y[0][3]):\n",
    "        count = 0\n",
    "        print(\"angry\")\n",
    "    elif(pred_y[0][1]>pred_y[0][0] and pred_y[0][1]>pred_y[0][2] and pred_y[0][1]>pred_y[0][3]):\n",
    "        count = 1\n",
    "        print(\"disgust\")\n",
    "    elif(pred_y[0][2]>pred_y[0][0] and pred_y[0][2]>pred_y[0][1] and pred_y[0][2]>pred_y[0][3]):\n",
    "        count = 2\n",
    "        print(\"sad\")\n",
    "    elif(pred_y[0][3]>pred_y[0][0] and pred_y[0][3]>pred_y[0][2] and pred_y[0][3]>pred_y[0][0]):\n",
    "        count = 3\n",
    "        print(\"happy\")\n",
    "    image = cv.putText(image, class_names[count],\n",
    "            (447,63), cv.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n",
    "    cv.imshow('Prediction', image)\n",
    "    \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bf7fa",
   "metadata": {
    "id": "f12bf7fa"
   },
   "outputs": [],
   "source": [
    "test_image=cv2.imread(\"Brain_tumor_dataset1/train/yes/Y1.jpg\")\n",
    "face = cv2.resize(test_image, (256,256))\n",
    "(b, g, r)=cv.split(face) \n",
    "img=cv.merge([r,g,b])\n",
    "a = np.squeeze(img)\n",
    "a = a[np.newaxis, :, :]\n",
    "pred_y = cnn.predict(a)\n",
    "if(pred_y[0][0]>pred_y[0][1]):\n",
    "    plt.title(\"Yes\")\n",
    "    plt.imshow(img)\n",
    "elif(pred_y[0][1]>pred_y[0][0]):\n",
    "    plt.title(\"No\")\n",
    "    plt.imshow(img)\n",
    "else:\n",
    "    plt.title(\"cannot be detected\")\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c768d773",
   "metadata": {
    "id": "c768d773"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image=cv.imread(\"facial_exp/train/angry/107.jpg\")\n",
    "face = cv.resize(test_image, (64,64))\n",
    "(b, g, r)=cv.split(face) \n",
    "img=cv.merge([r,g,b])\n",
    "a = np.squeeze(img)\n",
    "a = a[np.newaxis, :, :]\n",
    "pred_y = cnn.predict(a)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9ebe6",
   "metadata": {
    "id": "f2d9ebe6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b649c",
   "metadata": {
    "id": "f54b649c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
